{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to combine the proteins given by the CullPDB PISCES server [link] with the secondary structure information provided by DSSP [link]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cull PDB: PISCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proteins from the PDB can be queried based on criteria such as resolution, sequence identity, etc. It's possible (as of 20/03/2018) to download different lists [here](http://dunbrack.fccc.edu/Guoli/pisces_download.php)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSSP files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PISCES lists provide PDB ID's, but they do not have the secondary structure information. To get that, you need to download the DSSP information from the PDB. This can be done directly [here](http://swift.cmbi.ru.nl/gv/dssp/).\n",
    "\n",
    "By syncing the database locally, the individual \\*.dssp files can be parsed by the script [here](https://gist.github.com/dillondaudert/94785e9cc0318ac69243c6283da3a032)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "The rest of this notebook will assume that a list downloaded from PISCES as well as some number of .csv files containing the parsed DSSP data exist in a `data/dssp` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we want to do a join on the PDB id field of the PISCES and DSSP datasets. Since these are both in either tab-separated or csv format, Pandas is an ideal candidate for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "datadir = str(Path(Path.home(), \"data\", \"dssp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpdb_df = pd.read_csv(datadir+\"/cullpdb_pc30_res2.5_R1.0_d180208_chains15102.txt\", delim_whitespace=True)\n",
    "print(cpdb_df.columns)\n",
    "print(cpdb_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some assumptions about this data\n",
    "all_length_5 = all(cpdb_df[\"IDs\"].str.len() == 5)\n",
    "cpdb_long_ids_df = cpdb_df[\"IDs\"][cpdb_df[\"IDs\"].str.len() != 5]\n",
    "# All IDs are unique\n",
    "unique_ids = len(cpdb_df[\"IDs\"]) == len(cpdb_df[\"IDs\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dssp_1_df = pd.read_csv(datadir+\"/dssp_1.csv\")\n",
    "print(dssp_1_df.columns)\n",
    "print(dssp_1_df[\"dssp_id\"][0], dssp_1_df[\"seq\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some assumptions about this data\n",
    "all_length_4 = all(dssp_1_df[\"dssp_id\"].str.len() == 4)\n",
    "unique_ids = len(dssp_1_df[\"dssp_id\"]) == len(dssp_1_df[\"dssp_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to concatenate the two datasets, joining on the two id's. Since the cpdb data is a subset of the dssp data, we join on the cpdb id field (after taking the first 4 characters only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dataframe consisting of only the first 4 characters of each cpdb id\n",
    "cpdb_ids = cpdb_df[\"IDs\"].str[0:4].str.lower()\n",
    "print(cpdb_ids.loc[0:5])\n",
    "# check if these are still unique\n",
    "print(len(cpdb_ids) == len(cpdb_ids.unique()))\n",
    "# only take the unique entries\n",
    "cpdb_ids = cpdb_ids[~cpdb_ids.duplicated()]\n",
    "print(cpdb_ids.is_unique)\n",
    "\n",
    "# get the cpdb dataset with only unique, 4-letter IDs\n",
    "cpdb_df_unique = cpdb_df[~cpdb_df[\"IDs\"].duplicated()]\n",
    "\n",
    "cpdb_df_unique[\"dssp_id\"] = cpdb_ids\n",
    "cpdb_df_unique.drop(labels=[\"IDs\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on CPDB chains and DSSP ids\n",
    "The PISCES server checks each CHAIN of a PDB entry individually. As such, the cpdb IDs may contain all or only some of the chains of a particular PDB entry. On the other hand, the DSSP outputs a single file / entry per PDB ID, which will include (*I assume*) all of the chains for that entry.\n",
    "\n",
    "For simplicity, I will assume that if a particular 4-letter PDB entry appears at least once in the cpdb IDs, then the entire entry is acceptable to put in the dataset. This means that I can just take the unique 4-letter IDs from the cpdb_df and do an inner join with the dssp_dfs. Once this is done for all the dssp files, the resulting dataframe will contain the sequences, secondary structure, and other features of the PDB entries specified by the PISCES cull pdb list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two datasets\n",
    "merged_1 = dssp_1_df.merge(cpdb_df_unique, how=\"inner\", on=\"dssp_id\")\n",
    "print(merged_1.columns)\n",
    "print(len(merged_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do this for all dssp files\n",
    "merged_frames = []\n",
    "for i in range(1,12):\n",
    "    dssp_df = pd.read_csv(datadir+\"/dssp_%d.csv\" % i)\n",
    "    merged = dssp_df.merge(cpdb_df_unique, how=\"inner\", on=\"dssp_id\")\n",
    "    print(len(merged))\n",
    "    merged_frames.append(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged = pd.concat(merged_frames)\n",
    "print(len(all_merged))\n",
    "print(all_merged[\"dssp_id\"].is_unique)\n",
    "# Write out the sequence and secondary structure to a file\n",
    "all_merged[[\"dssp_id\", \"seq\", \"ss\"]].to_csv(datadir+\"/cpdb_dssp_%d.csv\" % len(all_merged), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating .TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
